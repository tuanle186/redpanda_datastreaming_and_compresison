{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d64da14c-38bc-45e3-9245-e8624fc73b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "import pandas as pd\n",
    "from confluent_kafka import Consumer, KafkaException, KafkaError\n",
    "# from src.server_consumer.MultiSensorDataGrouper import MultiSensorDataGrouper, load_data\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cbb7296b-eb6c-4399-a946-3212c7d9ee69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class MultiSensorDataGrouper:\n",
    "    def __init__(self, epsilon, window_size=100):\n",
    "        self.epsilon = epsilon\n",
    "        self.window_size = window_size\n",
    "        self.index_structure = {}  # Dictionary to store the index structure\n",
    "\n",
    "    # Bucket creation for a signal\n",
    "    def create_buckets(self, signal, epsilon):\n",
    "        \"\"\"\n",
    "        Create buckets for a given signal based on the epsilon threshold.\n",
    "        \n",
    "        :param signal: Array or list of signal values.\n",
    "        :param epsilon: Tolerance for bucketing.\n",
    "        :return: List of buckets, each containing the average value and original values.\n",
    "        \"\"\"\n",
    "        buckets = []\n",
    "        current_bucket = []\n",
    "        min_value, max_value = float('inf'), float('-inf')\n",
    "        \n",
    "        for value in signal:\n",
    "            current_bucket.append(value)\n",
    "            min_value = min(min_value, value)\n",
    "            max_value = max(max_value, value)\n",
    "\n",
    "            if max_value - min_value > 2 * epsilon:\n",
    "                average = (min_value + max_value) / 2\n",
    "                buckets.append((average, current_bucket))  # Store average and len\n",
    "                current_bucket = []\n",
    "                min_value, max_value = float('inf'), float('-inf')\n",
    "\n",
    "        if current_bucket:\n",
    "            average = (min_value + max_value) / 2\n",
    "            buckets.append((average, current_bucket))\n",
    "        \n",
    "        return buckets\n",
    "\n",
    "    def static_group(self, df, base_moteid, attribute):\n",
    "        \"\"\"\n",
    "        Perform static grouping on the DataFrame using a specified base moteid and a single attribute.\n",
    "\n",
    "        :param df: DataFrame containing sensor data with 'timestamp', 'moteid', and various attributes.\n",
    "        :param base_moteid: The moteid to use as the base for grouping.\n",
    "        :param attribute: The attribute (column) to group and calculate ratios for.\n",
    "        :return: Tuple of base signals, ratio signals, and total memory cost.\n",
    "        \"\"\"\n",
    "        base_signals = []\n",
    "        ratio_signals = {}\n",
    "\n",
    "        # Get the base signal for the specified moteid\n",
    "        base_signal_df = df[df['moteid'] == base_moteid].set_index('timestamp')[attribute].reset_index()\n",
    "\n",
    "        # Iterate over unique moteids except the base moteid\n",
    "        for moteid in df['moteid'].unique():\n",
    "            if moteid == base_moteid:\n",
    "                continue\n",
    "\n",
    "            # Get the signal for the current moteid\n",
    "            signal_df = df[df['moteid'] == moteid].set_index('timestamp')[attribute].reset_index()\n",
    "\n",
    "            # Merge the base and signal DataFrames on timestamp\n",
    "            merged_df = base_signal_df.merge(signal_df, on='timestamp', suffixes=('_base', '_signal'), how='inner')\n",
    "\n",
    "            # Filter out rows with NaN values in either column\n",
    "            merged_df = merged_df.dropna()\n",
    "\n",
    "            # Calculate the ratio if both base and signal exist\n",
    "            if f'{attribute}_base' in merged_df and f'{attribute}_signal' in merged_df:\n",
    "                # Calculate the ratio\n",
    "                merged_df[f'ratio_{attribute}'] = merged_df[f'{attribute}_signal'] / merged_df[f'{attribute}_base']\n",
    "\n",
    "                # Create buckets for the calculated ratio\n",
    "                ratio_buckets = self.create_buckets(merged_df[f'ratio_{attribute}'], 0.1)\n",
    "                ratio_signals[moteid] = ratio_buckets\n",
    "\n",
    "        # Create buckets for the base signal\n",
    "        base_buckets = self.create_buckets(base_signal_df[attribute], self.epsilon)\n",
    "        base_signals.append((base_moteid, base_buckets))\n",
    "\n",
    "        total_memory = self.calculate_memory_cost(base_signals, ratio_signals)\n",
    "        return base_signals, ratio_signals, total_memory\n",
    "\n",
    "    # Calculate the memory cost of compression (number of buckets)\n",
    "    def calculate_memory_cost(self, base_signals, ratio_signals):\n",
    "        \"\"\"\n",
    "        Calculate the memory cost based on the number of buckets.\n",
    "\n",
    "        :param base_signals: Base signals with their buckets.\n",
    "        :param ratio_signals: Ratio signals with their buckets.\n",
    "        :return: Total memory cost (number of buckets).\n",
    "        \"\"\"\n",
    "        memory_cost = 0\n",
    "\n",
    "        # Memory cost for base signals\n",
    "        for base_buckets in base_signals:\n",
    "            memory_cost += len(base_buckets[1])  # len of bucket values\n",
    "\n",
    "        # Memory cost for ratio signals\n",
    "        for ratio_buckets in ratio_signals.values():\n",
    "            memory_cost += len(ratio_buckets)\n",
    "\n",
    "        return memory_cost\n",
    "\n",
    "    # Function to reconstruct signal from buckets\n",
    "    def reconstruct_signal(self, buckets, base_signal=None):\n",
    "        \"\"\"\n",
    "        Reconstruct the signal from its buckets by replacing each value with the average of the bucket.\n",
    "\n",
    "        :param buckets: List of buckets (average, original values).\n",
    "        :param base_signal: Optional base signal for reconstructing based on base.\n",
    "        :return: Reconstructed signal.\n",
    "        \"\"\"\n",
    "        reconstructed_signal = []\n",
    "\n",
    "        for average, values in buckets:\n",
    "            if base_signal is not None:\n",
    "                # Reconstruct based on the base signal and average\n",
    "                base_value = base_signal[len(reconstructed_signal)]  # Use modulo for indexing\n",
    "#                 print(base_value, average, values)\n",
    "                reconstructed_signal.extend([base_value * average] * len(values))  # Multiply by the average ratio\n",
    "            else:\n",
    "                reconstructed_signal.extend([average] * len(values))  # Use average only for standard reconstruction\n",
    "        \n",
    "        return reconstructed_signal\n",
    "\n",
    "    def plot_signals(self, original_signals, compressed_buckets):\n",
    "        \"\"\"\n",
    "        Plot the original and compressed signals in a single plot for comparison.\n",
    "\n",
    "        :param original_signals: List of original signals.\n",
    "        :param compressed_buckets: List of compressed signals (buckets).\n",
    "        \"\"\"\n",
    "        num_signals = len(original_signals)\n",
    "\n",
    "        # Create a single figure for all signals\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        for i, (original, buckets) in enumerate(zip(original_signals, compressed_buckets)):\n",
    "            compressed_signal = self.reconstruct_signal(buckets)\n",
    "\n",
    "            # Plot original signal\n",
    "            plt.plot(original, linestyle='-', color=f'C{i}')\n",
    "\n",
    "            # Plot compressed signal\n",
    "            plt.plot(compressed_signal, linestyle='--', color=f'C{i}', alpha=0.8)\n",
    "\n",
    "        plt.title('Original and Compressed Signals')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Signal Value')\n",
    "        plt.legend(loc='best')  # Automatically adjusts to best position\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def extract_signals(self, df, base_moteid, attribute):\n",
    "        \"\"\"\n",
    "        Extract the base signal and other related signals for plotting.\n",
    "\n",
    "        :param df: DataFrame containing sensor data with 'timestamp', 'moteid', and various attributes.\n",
    "        :param base_moteid: The moteid to use as the base for extracting signals.\n",
    "        :param attribute: The attribute (column) to extract signals for (e.g., temperature, humidity).\n",
    "        :return: Tuple of base signal, list of other signals, and timestamps.\n",
    "        \"\"\"\n",
    "        # Extract base signal\n",
    "        base_signal_df = df[df['moteid'] == base_moteid].set_index('timestamp')[attribute].reset_index()\n",
    "\n",
    "        # Extract other signals (all moteids except the base)\n",
    "        other_signals = []\n",
    "        timestamps = base_signal_df['timestamp'].values\n",
    "\n",
    "        for moteid in df['moteid'].unique():\n",
    "            if moteid == base_moteid:\n",
    "                continue\n",
    "\n",
    "            # Extract signal for the current moteid\n",
    "            signal_df = df[df['moteid'] == moteid].set_index('timestamp')[attribute].reset_index()\n",
    "\n",
    "            # Merge base signal and current signal on timestamp to align them\n",
    "            merged_df = base_signal_df.merge(signal_df, on='timestamp', suffixes=('_base', '_signal'), how='inner')\n",
    "\n",
    "            # If timestamps align, extract the signal for plotting\n",
    "            if not merged_df.empty:\n",
    "                other_signals.append(merged_df[f'{attribute}_signal'].values)\n",
    "\n",
    "        # Return the base signal and list of other signals\n",
    "        base_signal = base_signal_df[attribute].values\n",
    "        return base_signal, other_signals, timestamps\n",
    "\n",
    "    def plot_signals_single(self, original_signals, reconstructed_signals):\n",
    "        \"\"\"\n",
    "        Plot original vs reconstructed signals for each signal in separate subplots.\n",
    "\n",
    "        :param original_signals: List of original signals.\n",
    "        :param reconstructed_signals: List of reconstructed signals.\n",
    "        \"\"\"\n",
    "        num_signals = len(original_signals)\n",
    "\n",
    "        fig, axes = plt.subplots(num_signals, 1, figsize=(10, 5 * num_signals))\n",
    "        if num_signals == 1:\n",
    "            axes = [axes]  # Handle single plot case\n",
    "\n",
    "        for i, (original, reconstructed) in enumerate(zip(original_signals, reconstructed_signals)):\n",
    "            # Plot original vs reconstructed\n",
    "            axes[i].plot(original, label='Original Signal', color='blue')\n",
    "            axes[i].plot(reconstructed, label='Reconstructed Signal', color='orange', linestyle='--')\n",
    "            axes[i].set_title(f'Signal {i + 1}')\n",
    "            axes[i].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        \n",
    "def load_data(file_path):\n",
    "    # Load the data from a text file without parsing dates upfront\n",
    "    df = pd.read_csv(file_path, sep=' ', header=None,\n",
    "                     names=['date', 'time', 'epoch', 'moteid', 'temperature', 'humidity', 'light', 'voltage'])\n",
    "    \n",
    "    # Combine 'date' and 'time' columns into a single datetime column\n",
    "    df['date_time'] = pd.to_datetime(df['date'] + ' ' + df['time'], errors='coerce')\n",
    "    \n",
    "    # Drop rows where 'date_time' couldn't be parsed correctly\n",
    "    df.dropna(subset=['date_time'], inplace=True)\n",
    "    \n",
    "    # Drop the original 'date' and 'time' columns\n",
    "    df.drop(columns=['date', 'time'], inplace=True)\n",
    "    \n",
    "    # Sort by 'date_time'\n",
    "    df.sort_values(by='date_time', inplace=True)\n",
    "    \n",
    "    # Drop rows where other columns contain NaN\n",
    "    df.dropna(subset=['moteid', 'temperature', 'humidity', 'voltage', 'light'], inplace=True)\n",
    "    df['moteid'] = df['moteid'].astype(int)\n",
    "    df['timestamp'] = df['date_time'].dt.floor('30s')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "950a221f-cb31-4673-9d7a-12e6223d5919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "file_path = '../data/raw/data.txt'\n",
    "\n",
    "# Load the data\n",
    "sensor_data = load_data(file_path)\n",
    "# Set the date_time as the index\n",
    "df = load_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fbe5b98b-3f47-4fad-8cbf-c27669214e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract the base signal and other related signals\n",
    "base_moteid = 1  # Use moteid 1 as the base\n",
    "attribute = 'temperature'  # Attribute to analyze\n",
    "attributes = ['temperature', 'humidity', 'light', 'voltage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "85c63d39-84a5-412b-b971-b4a039eb584b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing attribute: temperature\n",
      "Memory cost after compression for temperature: 3012 buckets\n",
      "Processing attribute: humidity\n",
      "Memory cost after compression for humidity: 3518 buckets\n",
      "Processing attribute: light\n",
      "Memory cost after compression for light: 87827 buckets\n",
      "Processing attribute: voltage\n",
      "Memory cost after compression for voltage: 59 buckets\n",
      "Total memory cost after compression: 94416 buckets\n"
     ]
    }
   ],
   "source": [
    "# Step 0: Define the list of attributes and the base mote ID\n",
    "attributes = ['temperature', 'humidity', 'light', 'voltage']\n",
    "base_moteid = 1  # Use moteid 1 as the base\n",
    "\n",
    "# Initialize the total memory cost\n",
    "total_memory_cost = 0\n",
    "\n",
    "for attribute in attributes:\n",
    "    print(f'Processing attribute: {attribute}')\n",
    "    \n",
    "    # Step 1: Extract the base signal and other related signals for the current attribute\n",
    "    base_signal, other_signals, timestamps = grouper.extract_signals(df, base_moteid, attribute)\n",
    "    \n",
    "    # Step 2: Perform static grouping to get compression buckets for the current attribute\n",
    "    base_signals, ratio_signals, memory_cost = grouper.static_group(df, base_moteid, attribute)\n",
    "    \n",
    "    # Accumulate the total memory cost\n",
    "    total_memory_cost += memory_cost\n",
    "    \n",
    "    # Step 3: Reconstruct the compressed signals for the current attribute\n",
    "    # Reconstruct the base signal from its compressed buckets\n",
    "    reconstructed_base_signal = grouper.reconstruct_signal(base_signals[0][1])\n",
    "    \n",
    "    # Reconstruct the other signals from their ratio buckets\n",
    "    reconstructed_other_signals = []\n",
    "    for moteid, ratio_buckets in ratio_signals.items():\n",
    "        reconstructed_signal = grouper.reconstruct_signal(ratio_buckets, base_signal)\n",
    "        reconstructed_other_signals.append(reconstructed_signal)\n",
    "    \n",
    "    # Step 4: Plot original vs reconstructed signals (if needed)\n",
    "    # Combine the base signal and other signals into one list for plotting\n",
    "    original_signals = [base_signal] + other_signals\n",
    "    reconstructed_signals = [reconstructed_base_signal] + reconstructed_other_signals\n",
    "    \n",
    "    # Optionally, add your plotting code here\n",
    "    \n",
    "    # Output the memory cost for the current attribute\n",
    "    print(f'Memory cost after compression for {attribute}: {memory_cost} buckets')\n",
    "\n",
    "# Output the total memory cost after processing all attributes\n",
    "print(f'Total memory cost after compression: {total_memory_cost} buckets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d13701-48c4-423b-8a47-a8fbead46dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot original and reconstructed signals\n",
    "grouper.plot_signals_single(original_signals, reconstructed_signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5e991ee5-35a1-4b89-ac8f-97164222f5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_data():\n",
    "    try:\n",
    "        # Load data for compression\n",
    "        df = load_data('../data/raw/data.txt')  # Ensure this function returns a pandas DataFrame\n",
    "\n",
    "        # List of attributes to process\n",
    "        attributes = ['temperature', 'humidity', 'light', 'voltage']\n",
    "\n",
    "        # Dictionary to store compressed data for each attribute\n",
    "        compressed_data = {}\n",
    "\n",
    "        # Define the base mote ID\n",
    "        base_moteid = 1  # Adjust this as needed\n",
    "\n",
    "        # Initialize your grouper object (ensure this class is defined and imported)\n",
    "        # grouper = Grouper()  # Adjust this if your grouper requires initialization parameters\n",
    "\n",
    "        for attribute in attributes:\n",
    "            # Step 1: Extract the base signal and other related signals\n",
    "            base_signal, other_signals, timestamps = grouper.extract_signals(\n",
    "                df, base_moteid, attribute\n",
    "            )\n",
    "\n",
    "            # Step 2: Perform static grouping to get compression buckets\n",
    "            base_signals, ratio_signals, total_memory_cost = grouper.static_group(\n",
    "                df, base_moteid, attribute\n",
    "            )\n",
    "\n",
    "            # Step 3: Reconstruct the compressed signals\n",
    "            # Reconstruct the base signal from its compressed buckets\n",
    "            reconstructed_base_signal = grouper.reconstruct_signal(base_signals[0][1])\n",
    "\n",
    "            # Reconstruct the other signals from their ratio buckets\n",
    "            reconstructed_other_signals = {}\n",
    "            for moteid, ratio_buckets in ratio_signals.items():\n",
    "                reconstructed_signal = grouper.reconstruct_signal(\n",
    "                    ratio_buckets, reconstructed_base_signal\n",
    "                )\n",
    "                reconstructed_other_signals[moteid] = reconstructed_signal\n",
    "\n",
    "            # Combine the base signal and other signals\n",
    "            original_signals = [base_signal] + other_signals\n",
    "            reconstructed_signals = [reconstructed_base_signal] + list(reconstructed_other_signals.values())\n",
    "\n",
    "            # Store the compressed and reconstructed data\n",
    "            compressed_data[attribute] = {\n",
    "                'original_signals': original_signals,\n",
    "                'reconstructed_signals': reconstructed_signals,\n",
    "                'timestamps': timestamps,\n",
    "                'total_memory_cost': total_memory_cost,\n",
    "            }\n",
    "\n",
    "            # Output the total memory cost\n",
    "            print(f'Attribute: {attribute}')\n",
    "            print(f'Total memory cost after compression: {total_memory_cost} buckets')\n",
    "\n",
    "            # **Modifications Start Here**\n",
    "\n",
    "            # Function to convert numpy types to native Python types\n",
    "            def convert_numpy_types(obj):\n",
    "                if isinstance(obj, np.ndarray):\n",
    "                    return obj.tolist()\n",
    "                elif isinstance(obj, np.integer):\n",
    "                    return int(obj)\n",
    "                elif isinstance(obj, np.floating):\n",
    "                    return float(obj)\n",
    "                elif isinstance(obj, dict):\n",
    "                    return {convert_numpy_types(k): convert_numpy_types(v) for k, v in obj.items()}\n",
    "                elif isinstance(obj, (list, tuple)):\n",
    "                    return [convert_numpy_types(item) for item in obj]\n",
    "                else:\n",
    "                    return obj\n",
    "\n",
    "            # Convert base_signals and ratio_signals to JSON-serializable formats\n",
    "            processed_base_signals = []\n",
    "            for moteid, signal_data in base_signals:\n",
    "                processed_moteid = int(moteid)\n",
    "                processed_signal_data = convert_numpy_types(signal_data)\n",
    "                processed_base_signals.append([processed_moteid, processed_signal_data])\n",
    "\n",
    "            processed_ratio_signals = {}\n",
    "            for moteid, signal_data in ratio_signals.items():\n",
    "                processed_moteid = str(int(moteid))\n",
    "                processed_signal_data = convert_numpy_types(signal_data)\n",
    "                processed_ratio_signals[processed_moteid] = processed_signal_data\n",
    "\n",
    "            # Optionally, write the compressed data to files\n",
    "            with open(f'compressed_{attribute}_data.txt', 'w') as file:\n",
    "                file.write(json.dumps({\n",
    "                    'base_signals': processed_base_signals,\n",
    "                    'ratio_signals': processed_ratio_signals,\n",
    "                    'total_memory_cost': total_memory_cost\n",
    "                }) + '\\n')\n",
    "\n",
    "            with open(f'reconstructed_{attribute}_data.txt', 'w') as file:\n",
    "                for moteid, signal in zip([base_moteid] + list(ratio_signals.keys()), reconstructed_signals):\n",
    "                    file.write(json.dumps({\n",
    "                        'moteid': int(moteid),\n",
    "                        'attribute': attribute,\n",
    "                        'signal': convert_numpy_types(signal),\n",
    "                        'timestamps': convert_numpy_types(timestamps)\n",
    "                    }) + '\\n')\n",
    "\n",
    "        # You can add code here to perform any additional tasks, like plotting or further analysis.\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in compress_data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "28276ff1-742a-4fa6-95b3-d4f206d57adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute: temperature\n",
      "Total memory cost after compression: 3012 buckets\n",
      "Attribute: humidity\n",
      "Total memory cost after compression: 3518 buckets\n",
      "Attribute: light\n",
      "Total memory cost after compression: 87827 buckets\n",
      "Attribute: voltage\n",
      "Total memory cost after compression: 59 buckets\n"
     ]
    }
   ],
   "source": [
    "compress_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8aae1a-af68-44c6-9769-9c40339949cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
